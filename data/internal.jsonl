{"instruction":"Explain or adapt this Rust snippet from src/main.rs","input":"","output":"use anyhow::Result;\nuse std::net::SocketAddr;\nuse std::sync::Arc;\nuse tracing::info;\n\nmod api;\nmod core;\nmod training;\nmod wasm;\n\nuse crate::api::routes::create_api_router;\nuse crate::wasm::orchestrator::WasmOrchestrator;\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    // Initialize logging\n    tracing_subscriber::fmt::init();\n    \n    // Create and initialize the WASM orchestrator\n    let orchestrator = WasmOrchestrator::new()?;\n    let app_state = Arc::new(orchestrator);\n\n    // Create the API router\n    let app = create_api_router(app_state).await;\n\n    // Start the server\n    let addr = SocketAddr::from(([0, 0, 0, 0], 8080));\n    info!(\"User Automation Learning System listening on {}\", addr);\n    \n    axum::Server::bind(&addr)\n        .serve(app.into_make_service())\n        .await?;\n\n    Ok(())\n}\n"}
{"instruction":"Explain or adapt this Rust snippet from src/core/scraper.rs","input":"","output":"use anyhow::Result;\nuse octocrab::Octocrab;\nuse regex::Regex;\nuse serde::{Deserialize, Serialize};\nuse std::path::{Path, PathBuf};\nuse tokio::fs;\nuse std::process::Command;\nuse tokio::process::Command as TokioCommand;\nuse std::collections::HashMap;\nuse walkdir::WalkDir;\n\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct RepoMetadata {\n    pub name: String,\n    pub url: String,\n    pub clone_url: String,\n    pub stars: u32,\n    pub description: Option<String>,\n    pub language: Option<String>,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct CodeSnippet {\n    pub file: String,\n    pub functions: Vec<FunctionData>,\n    pub tests: Vec<TestData>,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct FunctionData {\n    pub name: String,\n    pub code: String,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct TestData {\n    pub code: String,\n    pub context: String,\n}\n\npub struct GitHubScraper {\n    client: Octocrab,\n    dataset_path: PathBuf,\n    raw_path: PathBuf,\n    processed_path: PathBuf,\n}\n\nimpl GitHubScraper {\n    pub async fn new(github_token: String) -> Result<Self> {\n        let client = Octocrab::builder()\n            .personal_token(github_token)\n            .build()?;\n\n        let dataset_path = PathBuf::from(\"./dataset\");\n        let raw_path = dataset_path.join(\"raw\");\n        let processed_path = dataset_path.join(\"processed\");\n\n        // Create directories\n        for path in &[&dataset_path, &raw_path, &processed_path] {\n            fs::create_dir_all(path).await?;\n        }\n\n        Ok(Self {\n            client,\n            dataset_path,\n            raw_path,\n            processed_path,\n        })\n    }\n\n    pub async fn scrape_web_automation_repos(&self) -> Result<Vec<RepoMetadata>> {\n        let search_queries = [\n            \"chromiumoxide rust web automation\",\n            \"thirtyfour selenium rust\",\n            \"headless_chrome rust automation\",\n            \"playwright rust browser\",\n            \"puppeteer rust automation\",\n            \"webdriver rust selenium\",\n            \"browser automation rust crate\",\n            \"web scraping rust async\",\n        ];\n\n        let mut repo_map = HashMap::new();\n\n        for query in &search_queries {\n            let repos = self.client\n                .search()\n                .repositories(query)\n                .language(\"rust\")\n                .stars(\">10\")\n                .sort(\"stars\")\n                .order(\"desc\")\n                .per_page(50)\n                .send()\n                .await?;\n\n            for item in repos.items {\n                if !repo_map.contains_key(&item.full_name) {\n                    repo_map.insert(item.full_name.clone(), RepoMetadata {\n                        name: item.full_name,\n                        url: item.html_url.to_string(),\n                        clone_url: item.clone_url.unwrap_or_default(),\n                        stars: item.stargazers_count,\n                        description: item.description,\n                        language: item.language,\n                    });\n                }\n            }\n\n            // Rate limiting pause\n            tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;\n        }\n\n        Ok(repo_map.into_values().collect())\n    }\n\n    pub async fn download_repo_content(&self, repo: &RepoMetadata) -> Result<bool> {\n        let repo_dir = self.raw_path.join(repo.name.replace('/', \"_\"));\n        \n        // Clone or update repository\n        let clone_result = if repo_dir.exists() {\n            TokioCommand::new(\"git\")\n                .args(&[\"pull\"])\n                .current_dir(&repo_dir)\n                .output()\n                .await\n        } else {\n            TokioCommand::new(\"git\")\n                .args(&[\"clone\", \"--depth\", \"1\", &repo.clone_url, &repo_dir.to_string_lossy()])\n                .output()\n                .await\n        };\n\n        if clone_result.is_err() {\n            return Ok(false);\n        }\n\n        // Find and process Rust files\n        let rust_files = self.find_rust_files(&repo_dir);\n        let relevant_code = self.extract_relevant_code(&rust_files).await?;\n\n        if relevant_code.is_empty() {\n            return Ok(false);\n        }\n\n        // Save extracted code\n        let output_file = self.raw_path.join(format!(\"{}.json\", repo.name.replace('/', \"_\")));\n        let output_data = serde_json::json!({\n            \"repo\": repo.name,\n            \"stars\": repo.stars,\n            \"description\": repo.description,\n            \"files\": relevant_code,\n            \"extracted_at\": chrono::Utc::now().to_rfc3339()\n        });\n\n        fs::write(&output_file, serde_json::to_string_pretty(&output_data)?).await?;\n\n        // Clean up cloned repo\n        fs::remove_dir_all(&repo_dir).await?;\n\n        Ok(true)\n    }\n\n    fn find_rust_files(&self, repo_dir: &Path) -> Vec<PathBuf> {\n        WalkDir::new(repo_dir)\n            .into_iter()\n            .filter_map(|e| e.ok())\n            .filter(|e| e.path().extension().map_or(false, |ext| ext == \"rs\"))\n            .map(|e| e.path().to_path_buf())\n            .collect()\n    }\n\n    async fn extract_relevant_code(&self, rust_files: &[PathBuf]) -> Result<Vec<CodeSnippet>> {\n        use tokio::task;\n        use futures::StreamExt;\n        use std::sync::Arc;\n\n        let automation_keywords = Arc::new([\n            \"chromiumoxide\", \"thirtyfour\", \"headless_chrome\", \"playwright\",\n            \"webdriver\", \"selenium\", \"browser\", \"page\", \"element\", \"click\",\n            \"navigate\", \"find_element\", \"wait_for\", \"screenshot\", \"execute_script\",\n            \"cookies\", \"local_storage\", \"session_storage\", \"iframe\", \"alert\"\n        ]);\n\n        // Create a stream of files to process in parallel\n        let mut tasks = futures::stream::iter(\n            rust_files.iter().cloned().map(|path| {\n                let keywords = automation_keywords.clone();\n                task::spawn(async move {\n                    if let Ok(content) = fs::read_to_string(&path).await {\n                        let has_automation_code = keywords.iter()\n                            .any(|&keyword| content.to_lowercase().contains(&keyword.to_lowercase()));\n\n                        if has_automation_code {\n                            let mut analyzer = CodeAnalyzer::new()?;\n                            let analyzed_functions = analyzer.analyze_code(&content)?;\n                            \n                            let functions = analyzed_functions.into_iter()\n                                .filter(|f| is_automation_function(&f.code))\n                                .map(|f| FunctionData {\n                                    name: f.name,\n                                    code: f.code,\n                                })\n                                .collect::<Vec<_>>();\n\n                            if !functions.is_empty() {\n                                return Ok(Some(CodeSnippet {\n                                    file: path.to_string_lossy().into_owned(),\n                                    functions,\n                                    tests: vec![], // Tests will be handled separately\n                                }));\n                            }\n                        }\n                    }\n                    Ok(None)\n                })\n            })\n        )\n        .buffer_unordered(num_cpus::get()); // Process files in parallel\n\n        let mut code_snippets = Vec::new();\n\n        for file_path in rust_files {\n            if let Ok(content) = fs::read_to_string(file_path).await {\n                let has_automation_code = automation_keywords.iter()\n                    .any(|&keyword| content.to_lowercase().contains(&keyword.to_lowercase()));\n\n                if has_automation_code {\n                    let functions = function_regex.captures_iter(&content)\n                        .filter(|cap| self.is_automation_function(&cap[0]))\n                        .map(|cap| FunctionData {\n                            name: cap[1].to_string(),\n                            code: cap[0].to_string(),\n                        })\n                        .collect::<Vec<_>>();\n\n                    let tests = test_regex.captures_iter(&content)\n                        .filter(|cap| self.is_automation_function(&cap[0]))\n                        .map(|cap| TestData {\n                            code: cap[0].to_string(),\n                            context: \"unit_test\".to_string(),\n                        })\n                        .collect::<Vec<_>>();\n\n                    if !functions.is_empty() || !tests.is_empty() {\n                        code_snippets.push(CodeSnippet {\n                            file: file_path.to_string_lossy().into_owned(),\n                            functions,\n                            tests,\n                        });\n                    }\n                }\n            }\n        }\n\n        Ok(code_snippets)\n    }\n\n    fn is_automation_function(&self, code: &str) -> bool {\n        let patterns = [\n            Regex::new(r\"(?i)browser|page|element\").unwrap(),\n            Regex::new(r\"(?i)click|navigate|find|wait\").unwrap(),\n            Regex::new(r\"(?i)chromiumoxide|thirtyfour|headless_chrome\").unwrap(),\n        ];\n\n        patterns.iter().any(|pattern| pattern.is_match(code))\n    }\n}\n"}
{"instruction":"Explain or adapt this Rust snippet from src/core/advanced_scraper.rs","input":"","output":"use anyhow::Result;\nuse fantoccini::{Client, ClientBuilder};\nuse rand::Rng;\nuse serde::{Deserialize, Serialize};\nuse std::{collections::HashMap, time::Duration};\nuse tokio::time::sleep;\nuse webdriver::capabilities::Capabilities;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ScrapingConfig {\n    pub user_agents: Vec<String>,\n    pub proxy_list: Vec<String>,\n    pub delay_range: (u64, u64), // milliseconds\n    pub max_retries: u32,\n    pub viewport_sizes: Vec<(u32, u32)>,\n    pub headers: HashMap<String, String>,\n}\n\nimpl Default for ScrapingConfig {\n    fn default() -> Self {\n        Self {\n            user_agents: vec![\n                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\".to_string(),\n                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\".to_string(),\n                \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\".to_string(),\n                // Add more modern user agents\n            ],\n            proxy_list: vec![], // To be filled from config\n            delay_range: (1000, 3000),\n            max_retries: 3,\n            viewport_sizes: vec![\n                (1920, 1080),\n                (1366, 768),\n                (1536, 864),\n                (1440, 900),\n            ],\n            headers: HashMap::new(),\n        }\n    }\n}\n\npub struct AdvancedScraper {\n    config: ScrapingConfig,\n    client: Option<Client>,\n    rng: rand::rngs::ThreadRng,\n}\n\nimpl AdvancedScraper {\n    pub fn new(config: Option<ScrapingConfig>) -> Self {\n        Self {\n            config: config.unwrap_or_default(),\n            client: None,\n            rng: rand::thread_rng(),\n        }\n    }\n\n    pub async fn init(&mut self) -> Result<()> {\n        let mut caps = Capabilities::new();\n        \n        // Set random user agent\n        let user_agent = self.get_random_user_agent();\n        caps.insert(\"goog:chromeOptions\".to_string(), serde_json::json!({\n            \"args\": [\n                \"--no-sandbox\",\n                \"--disable-dev-shm-usage\",\n                format!(\"--user-agent={}\", user_agent),\n                \"--disable-blink-features=AutomationControlled\", // Hide automation\n                \"--disable-infobars\",\n            ]\n        }));\n\n        // Set random viewport size\n        let (width, height) = self.get_random_viewport();\n        caps.insert(\"viewportSize\".to_string(), serde_json::json!({\n            \"width\": width,\n            \"height\": height\n        }));\n\n        // Initialize client with capabilities\n        self.client = Some(ClientBuilder::native()\n            .capabilities(caps)\n            .connect(\"http://localhost:4444\")\n            .await?);\n\n        Ok(())\n    }\n\n    pub async fn navigate(&mut self, url: &str) -> Result<String> {\n        let client = self.client.as_mut().expect(\"Client not initialized\");\n        \n        // Random delay before navigation\n        self.random_delay().await;\n\n        // Navigate with retry logic\n        let mut attempts = 0;\n        while attempts < self.config.max_retries {\n            match client.goto(url).await {\n                Ok(_) => {\n                    // Random scroll behavior\n                    self.simulate_human_scrolling(client).await?;\n                    \n                    // Get page content\n                    return Ok(client.source().await?);\n                }\n                Err(e) => {\n                    attempts += 1;\n                    if attempts == self.config.max_retries {\n                        return Err(e.into());\n                    }\n                    // Exponential backoff\n                    sleep(Duration::from_millis((1000 * attempts * attempts) as u64)).await;\n                }\n            }\n        }\n\n        Err(anyhow::anyhow!(\"Failed to navigate after retries\"))\n    }\n\n    pub async fn click(&mut self, selector: &str) -> Result<()> {\n        let client = self.client.as_mut().expect(\"Client not initialized\");\n        \n        // Random delay before click\n        self.random_delay().await;\n\n        // Move mouse naturally to element\n        self.simulate_human_mouse_movement(client, selector).await?;\n\n        // Click with retry logic\n        let mut attempts = 0;\n        while attempts < self.config.max_retries {\n            match client.find(fantoccini::Locator::Css(selector)).await?.click().await {\n                Ok(_) => return Ok(()),\n                Err(e) => {\n                    attempts += 1;\n                    if attempts == self.config.max_retries {\n                        return Err(e.into());\n                    }\n                    sleep(Duration::from_millis((1000 * attempts * attempts) as u64)).await;\n                }\n            }\n        }\n\n        Err(anyhow::anyhow!(\"Failed to click after retries\"))\n    }\n\n    async fn simulate_human_scrolling(&mut self, client: &mut Client) -> Result<()> {\n        // Get page height\n        let height = client.execute(\n            \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight);\",\n            vec![]\n        ).await?.as_f64().unwrap_or(0.0) as i64;\n\n        let mut current_pos = 0;\n        while current_pos < height {\n            // Random scroll amount\n            let scroll_amount = self.rng.gen_range(100..300);\n            current_pos += scroll_amount;\n\n            // Smooth scroll with easing\n            client.execute(\n                &format!(\n                    \"window.scrollTo({{ top: {}, behavior: 'smooth' }});\",\n                    current_pos\n                ),\n                vec![]\n            ).await?;\n\n            // Random pause between scrolls\n            sleep(Duration::from_millis(self.rng.gen_range(500..1500))).await;\n        }\n\n        Ok(())\n    }\n\n    async fn simulate_human_mouse_movement(&mut self, client: &mut Client, selector: &str) -> Result<()> {\n        // Get element position\n        let element = client.find(fantoccini::Locator::Css(selector)).await?;\n        let rect = element.rect().await?;\n\n        // Generate random bezier curve points for natural movement\n        let (start_x, start_y) = (0.0, 0.0); // Current mouse position\n        let (end_x, end_y) = (rect.x + (rect.width / 2.0), rect.y + (rect.height / 2.0));\n        \n        // Execute mouse movement along curve\n        client.execute(\n            &format!(\n                r#\"\n                const bezierCurve = (t, p0, p1, p2, p3) => {{\n                    const cX = 3 * (p1.x - p0.x),\n                          bX = 3 * (p2.x - p1.x) - cX,\n                          aX = p3.x - p0.x - cX - bX;\n                    const cY = 3 * (p1.y - p0.y),\n                          bY = 3 * (p2.y - p1.y) - cY,\n                          aY = p3.y - p0.y - cY - bY;\n                    const x = (aX * Math.pow(t, 3)) + (bX * Math.pow(t, 2)) + (cX * t) + p0.x;\n                    const y = (aY * Math.pow(t, 3)) + (bY * Math.pow(t, 2)) + (cY * t) + p0.y;\n                    return {{x, y}};\n                }};\n                const p0 = {{x: {}, y: {}}};\n                const p3 = {{x: {}, y: {}}};\n                const p1 = {{x: p0.x + {}, y: p0.y + {}}};\n                const p2 = {{x: p3.x - {}, y: p3.y - {}}};\n                for(let t = 0; t <= 1; t += 0.1) {{\n                    const pos = bezierCurve(t, p0, p1, p2, p3);\n                    // Would implement actual mouse movement here if we had lower level control\n                    console.log(`Mouse moved to: ${{pos.x}}, ${{pos.y}}`);\n                }}\n                \"#,\n                start_x, start_y, end_x, end_y,\n                self.rng.gen_range(50.0..150.0), self.rng.gen_range(20.0..100.0),\n                self.rng.gen_range(50.0..150.0), self.rng.gen_range(20.0..100.0)\n            ),\n            vec![]\n        ).await?;\n\n        Ok(())\n    }\n\n    fn get_random_user_agent(&mut self) -> String {\n        self.config.user_agents[self.rng.gen_range(0..self.config.user_agents.len())].clone()\n    }\n\n    fn get_random_viewport(&mut self) -> (u32, u32) {\n        self.config.viewport_sizes[self.rng.gen_range(0..self.config.viewport_sizes.len())]\n    }\n\n    async fn random_delay(&mut self) {\n        let delay = self.rng.gen_range(self.config.delay_range.0..self.config.delay_range.1);\n        sleep(Duration::from_millis(delay)).await;\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_scraper_initialization() {\n        let mut scraper = AdvancedScraper::new(None);\n        assert!(scraper.init().await.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_navigation_with_retries() {\n        let mut scraper = AdvancedScraper::new(None);\n        scraper.init().await.unwrap();\n        \n        // Test with a known reliable site\n        let result = scraper.navigate(\"https://example.com\").await;\n        assert!(result.is_ok());\n        assert!(result.unwrap().contains(\"Example Domain\"));\n    }\n}\n"}
{"instruction":"Explain or adapt this Rust snippet from src/core/types.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/core/mod.rs","input":"","output":"pub mod types;\npub mod scraper;    "}
{"instruction":"Explain or adapt this Rust snippet from src/core/analyzer.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/training/recorder.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/training/mod.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/training/orchestrator.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/training/scraper/mod.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/wasm/mod.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/wasm/orchestrator.rs","input":"","output":""}
{"instruction":"Explain or adapt this Rust snippet from src/api/routes.rs","input":"","output":"use crate::{\n    core::types::{AgentSpec, DOMEvent},\n    training::recorder::TaskRecorder,\n    wasm::orchestrator::WasmOrchestrator\n};\nuse serde_json::Value;\nuse std::sync::Arc;\nuse axum::{\n    extract::{Path, State},\n    http::StatusCode,\n    response::Json,\n    routing::{get, post},\n    Router,\n};\n\npub async fn create_api_router(orchestrator: Arc<WasmOrchestrator>) -> Router {\n    Router::new()\n        .route(\"/health\", get(|| async { \"OK\" }))\n        .route(\"/agents\", post(deploy_agent_handler))\n        .route(\"/learn_and_execute\", post(learn_and_execute_handler))\n        .with_state(orchestrator)\n}\n\nasync fn deploy_agent_handler(\n    State(state): State<Arc<WasmOrchestrator>>, \n    Json(payload): Json<AgentSpec>\n) -> StatusCode {\n    match state.deploy_agent(payload).await {\n        Ok(_) => StatusCode::CREATED,\n        Err(_) => StatusCode::INTERNAL_SERVER_ERROR,\n    }\n}\n\nasync fn learn_and_execute_handler(\n    State(orchestrator): State<Arc<WasmOrchestrator>>,\n    Json(events): Json<Vec<DOMEvent>>, // Simulate receiving a recorded session\n) -> (StatusCode, Json<Value>) {\n    let mut recorder = TaskRecorder::new();\n    for event in events {\n        recorder.record_event(event);\n    }\n    \n    let plan = recorder.generate_training_plan();\n    let orchestrator = orchestrator.as_ref();\n    match orchestrator.execute_training_plan(plan).await {\n        Ok(results) => (StatusCode::OK, Json(results)),\n        Err(e) => (\n            StatusCode::INTERNAL_SERVER_ERROR, \n            Json(serde_json::json!({\"error\": e.to_string()}))\n        ),\n    }\n}"}
{"instruction":"Explain or adapt this Rust snippet from src/api/mod.rs","input":"","output":"pub mod routes;\n"}
{"instruction":"Explain or adapt this Rust snippet from src/lib.rs","input":"","output":"pub mod api;\npub mod core;\npub mod training;\npub mod wasm;\n\n// Re-export commonly used types\npub use crate::core::types::*;\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::training::recorder::TaskRecorder;\n    use crate::training::scraper::GitHubScraper;\n    use crate::wasm::orchestrator::WasmOrchestrator;\n\n    #[tokio::test]\n    async fn test_github_scraper() {\n        if let Ok(token) = std::env::var(\"GITHUB_TOKEN\") {\n            let scraper = GitHubScraper::new(&token).await.expect(\"Failed to create scraper\");\n            scraper.initialize().await.expect(\"Failed to initialize scraper\");\n            \n            let repos = scraper.scrape_web_automation_repos().await.expect(\"Failed to scrape repos\");\n            assert!(!repos.is_empty(), \"Should find some repositories\");\n\n            // Test downloading first repo\n            if let Some(repo) = repos.first() {\n                let success = scraper.download_repo_content(repo).await.expect(\"Failed to download repo\");\n                assert!(success, \"Should successfully download and process repo\");\n            }\n        }\n    }\n\n    #[tokio::test]\n    async fn test_basic_training_pipeline() {\n        // 1. Create a recorder\n        let mut recorder = TaskRecorder::new();\n\n        // 2. Record some events\n        recorder.record_event(DOMEvent {\n            event_type: \"click\".to_string(),\n            element_tag: \"button\".to_string(),\n            xpath: \"/html/body/div/button[1]\".to_string(),\n        });\n\n        recorder.record_voice(VoiceAnnotation {\n            text: \"Click the submit button\".to_string(),\n            confidence: 0.95,\n        });\n\n        // 3. Generate a training plan\n        let plan = recorder.generate_training_plan();\n        assert!(!plan.steps.is_empty(), \"Training plan should have steps\");\n\n        // 4. Create orchestrator and execute plan\n        let orchestrator = WasmOrchestrator::new().expect(\"Should create orchestrator\");\n        let result = orchestrator.execute_training_plan(plan).await;\n        assert!(result.is_ok(), \"Should execute training plan without errors\");\n    }\n\n    #[tokio::test]\n    async fn test_github_scraper() {\n        if let Ok(token) = std::env::var(\"GITHUB_TOKEN\") {\n            let scraper = GitHubScraper::new(token).await.expect(\"Should create scraper\");\n            \n            // Test repository search\n            let repos = scraper.scrape_web_automation_repos().await.expect(\"Should find repos\");\n            assert!(!repos.is_empty(), \"Should find some repositories\");\n            \n            // Verify repo metadata\n            for repo in &repos {\n                assert!(!repo.name.is_empty(), \"Repository name should not be empty\");\n                assert!(repo.stars > 0, \"Repository should have stars\");\n                assert!(repo.clone_url.contains(\"github.com\"), \"Should be a GitHub URL\");\n            }\n            \n            // Test parallel repo processing\n            let mut successes = 0;\n            let mut futures = Vec::new();\n            \n            for repo in repos.iter().take(3) {\n                let scraper = scraper.clone();\n                let repo = repo.clone();\n                futures.push(tokio::spawn(async move {\n                    scraper.download_repo_content(&repo).await\n                }));\n            }\n            \n            for future in futures {\n                if let Ok(Ok(success)) = future.await {\n                    if success {\n                        successes += 1;\n                    }\n                }\n            }\n            \n            assert!(successes > 0, \"Should successfully process at least one repository\");\n        }\n    }\n    \n    #[tokio::test]\n    async fn test_code_analyzer() {\n        let mut analyzer = CodeAnalyzer::new().expect(\"Should create analyzer\");\n        \n        let test_code = r#\"\n        /// This is a test function\n        pub async fn test_automation(url: &str) -> Result<(), Error> {\n            let browser = Browser::new().await?;\n            let page = browser.new_page(url).await?;\n            \n            page.click(\"button\").await?;\n            page.type_text(\"input\", \"test\").await?;\n            \n            Ok(())\n        }\n        \"#;\n        \n        let analyzed = analyzer.analyze_code(test_code).expect(\"Should analyze code\");\n        assert_eq!(analyzed.len(), 1, \"Should find one function\");\n        \n        let func = &analyzed[0];\n        assert_eq!(func.name, \"test_automation\");\n        assert!(func.async_status, \"Should be async\");\n        assert_eq!(func.visibility, \"pub\");\n        assert!(func.documentation.is_some(), \"Should have documentation\");\n        assert_eq!(func.parameters.len(), 1, \"Should have one parameter\");\n        assert!(func.complexity > 1, \"Should have complexity > 1\");\n    }\n    \n    #[tokio::test]\n    async fn test_integrated_pipeline() {\n        if let Ok(token) = std::env::var(\"GITHUB_TOKEN\") {\n            let scraper = GitHubScraper::new(token).await.expect(\"Should create scraper\");\n            let mut recorder = TaskRecorder::new();\n            let orchestrator = WasmOrchestrator::new().expect(\"Should create orchestrator\");\n            \n            // 1. Scrape repositories\n            let repos = scraper.scrape_web_automation_repos().await.expect(\"Should find repos\");\n            assert!(!repos.is_empty(), \"Should find repositories\");\n            \n            // 2. Record example automation\n            recorder.record_event(DOMEvent {\n                event_type: \"click\".to_string(),\n                element_tag: \"button\".to_string(),\n                xpath: \"/html/body/div/button[1]\".to_string(),\n            });\n            \n            // 3. Generate and execute training plan\n            let plan = recorder.generate_training_plan();\n            let result = orchestrator.execute_training_plan(plan).await;\n            assert!(result.is_ok(), \"Should execute training plan\");\n        }\n    }\n}\n"}