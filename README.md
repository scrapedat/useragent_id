UserAgent.ID: A Protocol for Persistent, Verifiable AI Agents
Authored: Monday, July 15 2025
Version: 2.0
1.0 A New Foundation for Human-AI Collaboration
The rapid proliferation and increasing capability of Large Language Models (LLMs) have brought society to the precipice of a new technological paradigm. These models demonstrate a remarkable capacity for understanding, generation, and reasoning. However, their integration into the fabric of our digital lives has exposed a fundamental architectural deficiency: a "State-Action Gap." This gap represents the profound inability of current AI systems to perform meaningful, stateful, and auditable actions in the digital world on a user's behalf. This document presents a comprehensive blueprint for the UserAgent.ID protocol and its premier commercial implementation, Persistence.Digital—a multi-layered system designed to bridge this gap and provide a secure, persistent, and interoperable foundation for the future of human-AI collaboration.
1.1 The State-Action Gap in Modern AI
Today's interactions with AI, while powerful, are predominantly ephemeral and untrustworthy. They are constrained by a set of limitations that prevent them from evolving beyond sophisticated, stateless chatbots into true digital agents. The symptoms of the State-Action Gap are pervasive and readily identifiable :
 * Statelessness and Context Amnesia: The most immediate limitation is the lack of persistent memory. Users are forced to re-establish context in nearly every new session, repeating instructions, preferences, and historical details. An AI that cannot remember a user's goals from one hour to the next cannot be entrusted with long-term, multi-step tasks. This constant need for re-contextualization creates a high-friction user experience and severely limits the complexity of tasks that can be delegated.
 * Fragmentation and Lack of Interoperability: A user's "relationship" with an AI is siloed within the application where it is hosted. The context built within one application does not transfer to another, creating a disjointed and fragmented digital existence. An AI assistant that helps draft an email has no awareness of the project management tool it relates to, nor the calendar event that prompted its creation. This fragmentation prevents the emergence of a holistic, integrated assistant that can operate across a user's entire digital ecosystem.
 * The Trust Deficit: The most critical symptom of the State-Action Gap is the profound lack of a trust framework. Users are justifiably hesitant to grant AI systems autonomy over meaningful tasks—such as managing finances, handling sensitive communications, or executing business processes—due to significant security concerns. There is no standardized mechanism for a user to grant specific, revocable permissions, no verifiable way to audit the actions taken on their behalf, and no guarantee that the agent is acting solely in their interest. Without a robust foundation of trust, AI will remain a passive tool rather than an active, delegated agent.
1.2 Vision: Persistent, Verifiable AI Agents
The UserAgent.ID protocol is architected to systematically solve these challenges by enabling the creation of "Persistent, Verifiable AI Agents." This vision reframes the concept of a digital assistant from a disposable tool to a stable, long-term identity for a user's digital proxy. The protocol establishes a universal standard that decouples a user's identity, context, and capabilities from any single AI model, application, or service provider.
When an LLM or any other digital service encounters a UserAgent.ID handle, such as myname.uaid, it is not merely retrieving a user profile. It is initiating a secure protocol to request a temporary, scoped, and auditable delegation of the user's digital agency. The handle serves as the key to a user's sovereign digital life—a persistent, encrypted environment where their preferences, interaction history, and learned skills ("automations") are securely stored and managed under their exclusive control. This architecture transforms any AI model from a stateless information processor into a stateful, persistent agent capable of executing complex, multi-step tasks over time, across applications, and within a high-trust framework. The ultimate vision is to establish UserAgent.ID as a fundamental, functional domain for the agentic web, much as the Domain Name System (DNS) serves as the foundation for the traditional web.
1.3 Core Philosophy: Utility, Identity, and User Sovereignty
The design and implementation of the UserAgent.ID protocol are guided by a clear and unwavering philosophy, articulated through three core principles. These principles inform every architectural decision, from the choice of cryptographic primitives to the design of the business model, ensuring the long-term integrity and utility of the ecosystem.
 * Utility Over Tokenization: The UserAgent.ID ecosystem is engineered for function, not financial speculation. Its value is derived exclusively from its utility in providing persistence, automation, and trust for AI agents. The protocol deliberately eschews a native cryptocurrency or token-based incentive model. This is a strategic decision to focus development on solving real-world problems and to appeal to mainstream and enterprise users who require stable, predictable, and transparent service models. The commercial framework is a value-based Software-as-a-Service (SaaS) subscription offered by service providers like Persistence.Digital, ensuring that revenue is directly tied to the value delivered to the user.
 * Identity as the Bedrock of Trust: A high-trust network cannot be built on a foundation of anonymity. To prevent abuse, spam, and the proliferation of malicious agents, the protocol's persistent features are grounded in real-world, verified identity. While basic registration of a UserAgent.ID handle is free and accessible, unlocking the ability to create persistent agents with stateful capabilities requires user identity verification (Know Your Customer, or KYC). This requirement, while introducing a point of friction, is a necessary trade-off for fostering an ecosystem where agents can be held accountable and can be trusted to perform financially or legally significant actions on a user's behalf.
 * User Sovereignty by Design: This principle asserts that the user must be the ultimate and sole cryptographic authority over their digital identity, data, and actions. This is not merely a policy statement but an architectural guarantee enforced by the protocol's design. Through the use of non-custodial key management systems, the user retains exclusive control over the private keys that represent their identity. All high-stakes actions require explicit, cryptographically signed consent from the user, ensuring that no action can be taken without their auditable approval. This principle of self-sovereign identity, drawn from the foundational concepts of Web3, ensures that the user's data and digital agency remain theirs, protected from access or control by service providers, AI models, or any other third party.
2.0 System Architecture: A Multi-Layered Blueprint
The UserAgent.ID protocol is specified as a hybrid, multi-layered system that leverages the most effective centralized and decentralized technologies to achieve its goals of persistence, security, and interoperability. Each layer serves a distinct purpose, building upon the one below it to create a comprehensive and robust architecture for agentic AI.
2.1 Layer 1: The UserAgent.ID Handle & Protocol Entrypoint
The entry point into the UserAgent.ID ecosystem is the handle—a human-readable, globally unique identifier formatted as username.uaid. This handle is designed for simplicity and ease of use, allowing users to invoke their persistent agent from any compatible interface, such as a chatbot or an application's tool-calling function.
However, the handle is more than a simple alias. It is the public-facing component of a more sophisticated decentralized identity structure. The protocol will specify a discovery service—potentially leveraging existing infrastructure like DNS with custom resource records (e.g., TXT records) or a dedicated, distributed registry—that resolves a .uaid handle to a Decentralized Identifier (DID) document. DIDs are a W3C standard for verifiable, decentralized digital identity, providing a globally unique URI that does not require a centralized registration authority.
The resolved DID document is a structured JSON object containing the essential cryptographic information required to interact with the user's agentic infrastructure. This includes the user's public keys for signature verification, service endpoints for their Persistence Vault, and communication protocols for the P2P fabric. This architecture elegantly connects the user-friendly handle to a robust, standards-based decentralized identity framework.
The protocol interaction is initiated when a participating LLM or application recognizes the username.uaid format. This triggers a specific, standardized tool-calling function, such as initiate_uaid_session(handle: string). The invocation of this function signals the start of a secure session, where the AI is granted temporary, permissioned access to the user's persistent context and automation capabilities, as defined by the subsequent layers of the protocol. In this model, UserAgent.ID becomes a functional domain for AI, providing a universal addressing and interaction scheme for the emerging agentic web.
2.2 Layer 2: The Identity & Consent Fabric
This layer serves as the cryptographic heart of the protocol, functioning as the user's "Digital Passport". It manages the public/private key pairs that underpin the user's sovereign identity and provides the mechanisms through which they grant explicit, verifiable consent for agent actions. The architectural choices in this layer are paramount, as they directly enforce the core principle of User Sovereignty by Design.
2.2.1 Technology Selection: A Recommendation for Web3Auth's MPC Architecture
The selection of an identity provider is a foundational decision that dictates the entire trust model of the system. The initial protocol specification identified both Privy and Web3Auth as potential candidates for managing the cryptographic keys associated with each UserAgent.ID handle. After a thorough analysis of their underlying architectures, Web3Auth is the definitive and superior choice for the protocol's identity layer.
This recommendation is not based on features alone, but on a fundamental architectural difference that aligns directly with the protocol's core philosophy. Privy is an excellent and versatile authentication toolkit that provides seamless user onboarding through various methods, including social logins, email, and crypto wallets. Its system is primarily built upon a robust JSON Web Token (JWT) architecture. In this model, upon successful authentication, Privy's servers issue short-lived access tokens and long-lived refresh tokens to the client. These tokens represent the user's authenticated session and are used to authorize actions. While highly secure and widely adopted, this model centralizes the root of trust with the token-issuing authority. The security of the session depends on the security of the JWTs and the integrity of the issuer.
Web3Auth, by contrast, is built on a fundamentally different paradigm: Multi-Party Computation (MPC). Specifically, it employs a Threshold Signature Scheme (TSS) to manage user keys. Instead of a single private key being held in one location (or held by a central provider), the user's key is generated as multiple independent mathematical "shares" distributed across several factors. A typical configuration is a 2-of-3 setup, where one share is stored on the user's device (e.g., in the browser's secure storage), one share is managed by the Web3Auth network and secured by the user's social login (e.g., Google), and a third share serves as a recovery method (e.g., a password, a different device, or a downloaded file).
The critical property of this MPC architecture is that the complete private key is never reconstructed in any single location, not even for a moment during transaction signing. Signatures are created through a distributed computation where each party uses its share to contribute to the final signature. To gain access or sign a message, a user must present a threshold of their shares (e.g., 2 out of 3). This makes the system non-custodial by design; the user, and only the user, has control over the necessary components to access their identity. Neither Web3Auth nor the service operator (Persistence.Digital) can unilaterally access the user's account or sign on their behalf, as they only control, at most, one share.
This architectural choice has a profound, cascading effect on the entire system's trust model. It elevates UserAgent.ID from a sophisticated single sign-on (SSO) system, where trust is placed in a central token issuer, to a true decentralized identity protocol, where trust is rooted in the user's own cryptographic keys. This directly and uncompromisingly enforces the principle of User Sovereignty. The selection of Web3Auth is not merely a choice of a login provider; it is the selection of an architectural paradigm that guarantees a zero-trust relationship between the user and the service provider. This is a powerful differentiator and a prerequisite for building a high-trust network for AI agents. Furthermore, Web3Auth provides a suite of SDKs that abstract this cryptographic complexity into familiar, user-friendly login flows, making this advanced security model accessible to a mainstream audience.
<br>
Table 1: Identity Provider Comparison (Web3Auth vs. Privy)
| Feature | Web3Auth | Privy | Analysis & Recommendation |
|---|---|---|---|
| Core Architecture | Multi-Party Computation (MPC) with Threshold Signature Schemes (TSS)  | JSON Web Token (JWT) based with Access/Refresh Tokens  | Web3Auth's MPC architecture is fundamentally decentralized, distributing key control. Privy's JWT model, while secure, centralizes trust in the token issuer. |
| Custodianship Model | Non-custodial. User controls a threshold of key shares. The full key is never held by any single party. | Provider-mediated. The user trusts the provider to issue and validate tokens correctly. | The non-custodial model of Web3Auth directly enforces the principle of User Sovereignty. |
| Security Model | Distributed trust. A compromise of a single factor (e.g., social login) is insufficient to compromise the account. | Centralized trust. Relies on the security of the issued tokens and the provider's infrastructure for revocation and session management. | Distributed trust provides superior resilience against single points of failure and attack vectors. |
| User Onboarding UX | Excellent. Supports social logins, email, passkeys, and traditional crypto wallets with a seamless, web2-like experience. | Excellent. Also supports a wide range of social, email, and wallet-based login methods for progressive onboarding. | Both providers excel at user experience, which is critical for adoption. This is not a key differentiator. |
| Developer SDKs | Comprehensive SDKs for Web (React, JS), Mobile (iOS, Android, React Native), and Gaming (Unity). | Comprehensive SDKs for Web (React), Mobile (React Native, iOS, Android), and various server-side languages. | Both providers offer strong developer tooling, facilitating integration. |
| Philosophical Alignment | High. The non-custodial, distributed trust model is in perfect alignment with the protocol's core vision of a user-sovereign agentic web. | Medium. Provides an excellent user experience and robust security but follows a more traditional, centralized trust model. | Web3Auth's architecture is not just a feature but a direct implementation of the project's core philosophy. |
| Recommendation | Selected. Web3Auth is chosen for its superior alignment with the protocol's principles of user sovereignty and decentralized trust, providing an architecturally enforced guarantee of user control. |  |  |
<br>
2.2.2 The Cryptographic Consent Flow: From User Intent to Verifiable Action
The "2-step verification" flow outlined in the initial whitepaper  is hereby formalized as the "Cryptographic Consent Flow." This mechanism is the cornerstone of the protocol's trust model, providing an immutable and non-repudiable link between user intent and system action. It transforms user approval from a simple checkbox click into a legally and technically binding cryptographic event. The process is inspired by the security and clarity of standards like Sign-In with Ethereum (EIP-4361), but is adapted for authorizing discrete actions rather than authenticating sessions.
The flow proceeds through five distinct stages:
 * Intent Formulation: An AI agent, operating within its secure execution session, determines that a high-stakes action is required to fulfill a user's request. A "high-stakes" action is defined as any operation that creates or modifies persistent data, establishes a new automation, accesses sensitive information, or incurs a financial cost. For example, the agent formulates the intent: "Create an automation to check my email for shipping updates every hour and store the results in my context vault."
 * Proposal Generation: The system translates the agent's intent into a structured, human-readable data object. This will be a JSON object that conforms to a strictly defined schema, ensuring consistency and machine-verifiability. The proposal contains all relevant parameters of the action, such as the automation's trigger, the steps to be executed, and the data to be accessed.
 * Challenge Delivery: The JSON proposal is delivered to the user for approval via a secure, out-of-band channel. This could be a push notification to a dedicated mobile application, a secure email, or an interface within the Persistence.Digital web application. The user is presented with a clear, unambiguous summary of the proposed action derived from the proposal data.
 * Cryptographic Signature: The user reviews the proposal. If they approve, they authorize the action by signing a message containing the cryptographic hash (e.g., SHA-256) of the canonicalized proposal JSON. This signature is generated using the user's UserAgent.ID private key, which is managed securely on their device by the Web3Auth SDK. The signature algorithm employed will be an asymmetric scheme such as RS256 (RSA Signature with SHA-256). The use of an asymmetric algorithm is critical, as it allows the system to verify the signature using the user's public key without ever needing access to the private key, which remains securely under the user's control.
 * Verification, Execution, and Logging: The signed approval is transmitted back to the system. The system's first step is to verify the signature against the user's public key, which is retrieved from their DID document. If and only if the signature is valid, two actions occur in parallel: the proposed action is dispatched to the appropriate layer for execution (e.g., the Automation Engine), and a record of the signed proposal is committed to the Verifiable Persistence Ledger (Layer 6).
This flow provides absolute non-repudiation. The user's cryptographic signature serves as undeniable proof that they consented to a specific, well-defined action at a particular point in time. This creates a verifiable audit trail that protects both the user and the service provider, establishing the high-trust foundation necessary for delegating meaningful tasks to AI agents.
2.3 Layer 3: The Secure Execution Environment
This layer is responsible for provisioning the ephemeral, sandboxed environment where an AI agent can safely operate tools, such as a web browser, on the user's behalf. The paramount challenge for this layer in a commercial service like Persistence.Digital is providing strong, scalable, and cost-effective isolation between the workloads of thousands of different tenants. Running code derived from potentially untrusted LLM outputs or interacting with the open web necessitates the highest possible level of security to prevent cross-tenant attacks and container escape vulnerabilities.
2.3.1 Isolation at the Hardware Level: A Recommendation for Firecracker MicroVMs
The original whitepaper describes a "Browser-as-a-Service" built on containers. While containers are efficient, their reliance on a shared host operating system kernel creates a significant attack surface. A vulnerability in the kernel could be exploited by a malicious process inside one container to "escape" and gain access to the host system, thereby compromising the data and processes of all other tenants on that host. In a multi-tenant environment where the service is executing arbitrary web automation, this risk is unacceptable.
To address this, an analysis of two leading strong-isolation technologies was conducted: gVisor and Firecracker.
 * gVisor is a user-space kernel written in Go. It creates a sandbox by intercepting system calls made by the containerized application and reimplementing the necessary kernel functionality within its own secure process, called the Sentry. This prevents the application from directly interacting with the host kernel, significantly reducing the attack surface. While gVisor offers a substantial security improvement over standard containers, it is not without trade-offs. It can introduce notable performance overhead, particularly for workloads that are I/O or system-call intensive, which is common in browser automation. Furthermore, the gVisor Sentry itself, while written in a memory-safe language, becomes a new and complex piece of software in the security-critical path.
 * Firecracker, in contrast, is a Virtual Machine Monitor (VMM) developed by AWS specifically for secure, multi-tenant serverless workloads. It utilizes the Linux Kernel-based Virtual Machine (KVM) to create extremely lightweight virtual machines, or "microVMs." Each microVM runs its own complete, isolated guest kernel. This provides a hardware-enforced security boundary, the strongest form of isolation available in modern cloud computing. Firecracker is minimalist by design; it exposes a minimal device model (only five emulated devices) to the guest, drastically reducing the attack surface compared to traditional VMMs like QEMU. Despite being full virtual machines, Firecracker microVMs are incredibly efficient, with a startup time of less than 125 milliseconds and a memory overhead of under 5 MiB, making them suitable for ephemeral, per-session workloads.
Based on this analysis, Firecracker is the definitive architectural choice for the Secure Execution Environment. For a service that must securely run browser automation tasks for a multitude of untrusting tenants, the hardware-virtualized isolation provided by microVMs offers an unparalleled security guarantee against cross-tenant interference.
This decision fundamentally reframes the operational challenge of the execution layer. The problem is no longer one of container security—hardening images, managing capabilities, and configuring seccomp profiles. Instead, it becomes a problem of microVM fleet management. The core engineering task shifts from securing a shared environment to efficiently orchestrating the lifecycle of thousands of disposable, fully isolated environments. This requires a robust orchestration system capable of provisioning, networking, monitoring, and terminating a vast number of ephemeral microVMs in response to user session demands. This approach, while operationally complex, builds the service on a foundation of maximal security and isolation, which is non-negotiable for a high-trust agentic platform. Frameworks such as firecracker-containerd can help bridge this gap by allowing Kubernetes to orchestrate Firecracker microVMs as if they were containers, combining the strong isolation of VMs with the mature ecosystem of container orchestration.
<br>
Table 2: Execution Environment Isolation Technology Comparison (Firecracker vs. gVisor)
| Criterion | Firecracker | gVisor | Analysis & Recommendation |
|---|---|---|---|
| Isolation Model | Hardware Virtualization (KVM). Each workload runs in a separate microVM with its own guest kernel. | Application Kernel / Syscall Interception. Workloads run in a sandbox that reimplements the Linux kernel in user-space. | Firecracker's model leverages hardware features for isolation, which is fundamentally stronger than a software-based approach. |
| Security Boundary | Hardware-enforced. The boundary is the hypervisor (KVM), a small, highly-audited component of the Linux kernel. | Software-enforced. The boundary is the gVisor Sentry process, a large and complex user-space application. | A hardware-enforced boundary provides a higher degree of assurance against escape vulnerabilities. |
| Attack Surface | Minimalist VMM. Exposes only 5 emulated devices to the guest, drastically reducing potential exploit vectors. | The gVisor Sentry must implement a surface area of approximately 200 Linux system calls to achieve broad compatibility. | Firecracker's minimalist design presents a significantly smaller and more defensible attack surface. |
| Performance | Near-native CPU performance. I/O operations incur a slight overhead due to crossing the VM boundary. | Low overhead for CPU-bound tasks. Can have significant overhead for I/O and syscall-heavy workloads like file system access. | For browser automation, which involves significant network and file I/O, Firecracker's more predictable performance profile is advantageous. |
| Startup Time | Extremely fast for a VM, typically <125ms, enabling on-demand, ephemeral use cases. | Sub-second, but generally slower than native containers. Not as optimized for rapid, high-frequency instantiation as Firecracker. | Firecracker's startup speed is a key enabler for provisioning a fresh, clean environment for every user session. |
| Maturity & Use Case | Battle-tested at massive scale as the foundation for AWS Lambda and AWS Fargate, purpose-built for multi-tenancy. | Proven in production as the technology behind Google Cloud Run and GKE Sandbox, designed for sandboxing untrusted containers. | Firecracker's origin and design are perfectly aligned with the requirements of the UserAgent.ID execution layer. |
| Recommendation | Selected. Firecracker is chosen for its superior, hardware-enforced isolation, which is the paramount concern for a secure, multi-tenant service running potentially untrusted browser automations. |  |  |
<br>
2.3.2 Environment Orchestration and Observability
The practical implementation of the Firecracker-based execution layer requires a sophisticated orchestration system. For each user session initiated via a UserAgent.ID handle, the orchestrator will perform the following lifecycle management steps:
 * Provisioning: A new Firecracker microVM is instantiated on demand. The orchestrator selects a host machine with available capacity and uses the Firecracker API to configure and start the microVM.
 * Booting: The microVM boots a minimal, custom-built Linux kernel and a read-only root file system. This file system is a pre-baked image containing only the necessary components: a lightweight desktop environment (XFCE with the Xvfb virtual frame buffer), Google Chrome with remote debugging enabled, a noVNC server for user observability, and the playwright-helper API bridge service.
 * Networking: The orchestrator attaches a virtual network interface to the microVM, connecting it to a virtual network. Strict network policies are applied at this stage, limiting ingress and egress traffic to only what is absolutely necessary for the automation task, thus preventing the environment from being used for malicious network activities.
 * Monitoring: Throughout the session, the orchestrator collects key metrics from the microVM, including CPU usage, memory consumption, network I/O, and the status of the automation processes running inside. This observability is crucial for resource management, performance tuning, and detecting anomalous behavior in a large-scale multi-tenant system.
 * Termination: Upon the completion of the user's task or the expiration of the session, the orchestrator terminates the microVM entirely. The entire environment, including the guest kernel and all in-memory state, is destroyed. This ensures that no data or state can leak between sessions and that every new session starts from a pristine, known-good state.
2.4 Layer 4: The Persistence Vault (A Persistence.Digital Implementation)
This layer represents the core commercial service offered by Persistence.Digital. It is the secure, user-owned "digital brain" that provides the long-term memory (Context) and reusable skills (Automations) for a user's persistent agent. All data within this vault is encrypted at rest using keys derived from the user's sovereign identity, making it inaccessible to the service operator.
2.4.1 The Context Engine: Vector Schema for Long-Term Memory
The create_user_embeddings function mentioned in the initial whitepaper is formalized here as the Context Engine. This engine is responsible for transforming the stream of a user's digital life into a searchable, long-term memory for their AI agent. The process involves chunking all relevant unstructured data—chat interactions, results of automation runs, content from browsed web pages, and user-provided documents—passing these chunks through a state-of-the-art text embedding model (e.g., OpenAI's text-embedding-3-small), and storing the resulting vectors in a dedicated vector database.
A naive implementation would simply store the vectors. However, to build a truly powerful and useful memory, a structured approach to vector storage is essential. Each data point in the vector database will conform to a well-defined schema, enabling powerful hybrid search capabilities that combine semantic similarity with structured metadata filtering.
The proposed schema for each vector point is as follows:
 * id: A Universally Unique Identifier (UUID) serving as the primary key for the data chunk.
 * vector: A high-dimensional array of floating-point numbers representing the semantic embedding of the content.
 * metadata: A JSON object containing structured, filterable information about the data chunk. This metadata is as important as the vector itself for effective retrieval.
   * user_id: The UserAgent.ID of the data's owner, used for multi-tenant data partitioning.
   * source_type: An enumerated string indicating the origin of the data (e.g., chat, automation_run, web_scrape, document_upload).
   * source_uri: A unique identifier for the source object (e.g., a chat session ID, an automation recipe ID, the URL of a scraped page).
   * timestamp: An ISO 8601 formatted timestamp indicating when the event occurred.
   * original_text: The raw text content of the chunk, allowing the LLM to access the ground truth data after retrieval.
   * access_control_tags: An array of user-defined or system-generated tags (e.g., private, work, finance, project-phoenix) that can be used for fine-grained access control and context scoping.
This schema design transforms the context vault from a simple vector store into a sophisticated knowledge base. An AI agent can now formulate complex queries such as, "Find memories related to 'Project Phoenix' from 'chat' sources in the last 30 days," which the system translates into a hybrid query that performs a semantic search on the vector space while simultaneously filtering on the source_type, timestamp, and access_control_tags metadata fields. This is a best practice for building production-grade Retrieval-Augmented Generation (RAG) systems and is essential for providing relevant, context-aware information to the agent.
2.4.2 The Automation Engine: A YAML Schema for Reusable Skills ("Recipes")
The reusable, user-approved workflows, or "recipes," are the skills that give an AI agent its ability to act. To ensure these skills are transparent, auditable, version-controllable, and easily generated or modified by an LLM, they will be stored as structured YAML documents rather than opaque scripts. This "automation-as-code" approach is inspired by modern CI/CD and workflow orchestration systems like GitHub Actions and Google Cloud Dataproc Workflows.
A well-defined YAML schema provides a declarative language for describing a workflow. The system's Automation Engine acts as an interpreter for this language, securely executing the defined steps. This prevents the execution of arbitrary, potentially malicious code and enables a clear separation of concerns between the definition of a workflow and its execution.
The proposed V1 schema for an automation recipe is as follows:
# automation_recipe.v1.yaml
name: "Hourly Shipping Update Checker"
description: "Checks Gmail for new shipping updates from specific senders and sends a summary to a Slack channel."
version: 1.0

trigger:
  type: schedule
  cron: "0 * * * *" # Run at the top of every hour.

steps:
  - id: check_email
    name: "Scan Gmail for new shipping notifications"
    action: browser.goto # A built-in action provided by the platform.
    inputs:
      url: "https://mail.google.com/mail/u/0/#search/from%3Ashipper%40example.com+is%3Aunread"
      
  - id: extract_data
    name: "Extract tracking information from new emails"
    action: llm.extract # A built-in action that uses an LLM for structured data extraction.
    inputs:
      prompt: "From the current page content, extract all shipping tracking numbers and their corresponding carriers. Only include information from unread emails."
      schema: # A JSON schema defining the desired output format.[span_70](start_span)[span_70](end_span)
        type: object
        properties:
          shipments:
            type: array
            items:
              type: object
              properties:
                tracking_number: { type: string }
                carrier: { type: string }
              required: [tracking_number, carrier]
              
  - id: post_to_slack
    name: "Post summary to the #logistics Slack channel"
    action: api.post # A generic action for making HTTP POST requests.
    inputs:
      url: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX"
      body:
        text: "Found new shipping updates: ${{ steps.extract_data.outputs.shipments }}" # Using an expression to reference output from a previous step.[span_71](start_span)[span_71](end_span)

This structured format provides numerous advantages :
 * Readability: The workflow is easy for humans to read and understand.
 * Composability: The system can provide a library of trusted, built-in actions (e.g., browser.goto, llm.extract, api.post) that can be composed into complex workflows.
 * Security: The Automation Engine interprets the YAML, not executes it. Inputs are treated as data, preventing code injection attacks. Actions like api.post can be subject to strict egress policies.
 * Interoperability: The use of expressions (${{... }}) to pass data between steps allows for complex data pipelines.
 * AI-Friendliness: This declarative format is an ideal target for LLMs to generate, analyze, and modify, allowing users to create complex automations using natural language.
2.5 Layer 5: The Decentralized Communication Fabric
This layer provides the real-time, low-latency, and secure communication backbone for the UserAgent.ID ecosystem. It enables direct peer-to-peer (P2P) communication between different AI agents, or between a user's client devices (e.g., a web browser or mobile app) and their active agent, without needing to route all messages through a centralized server. This is essential for interactive applications, collaborative agent workflows, and sending real-time notifications.
2.5.1 Protocol Selection: A Recommendation for libp2p
The initial whitepaper identified Hyperswarm and libp2p as potential technologies for this layer. Both are robust P2P networking stacks.
 * Hyperswarm is a high-performance networking stack built around a Kademlia-based Distributed Hash Table (DHT). It excels at peer discovery and establishing end-to-end encrypted Noise protocol streams, and it features a highly effective UDP hole-punching algorithm for traversing NATs.
 * libp2p is a modular P2P networking framework that serves as the networking layer for major decentralized projects like IPFS and Ethereum 2.0. Its key strength is its modularity and transport-agnosticism. It can operate over various transport protocols, including TCP, QUIC, WebSockets, and WebRTC, and it supports multiple peer discovery mechanisms, such as mDNS for local network discovery and a Kademlia DHT for global discovery.
While both are excellent technologies, libp2p is the recommended choice for the UserAgent.ID protocol. Its modularity and broad support for multiple transports provide maximum flexibility for the ecosystem. The ability to use WebSockets and WebRTC is particularly crucial, as it allows for direct, secure communication channels to be established from a standard web browser to a user's agent running in a microVM, without requiring any special client software. This flexibility and its wide adoption within the broader Web3 community make it a more strategic and future-proof choice.
2.5.2 Peer Discovery and Real-Time Agent Communication
When a user's agent session becomes active within its Firecracker microVM, it will initialize a libp2p node. This node is identified by a unique PeerID, which is cryptographically derived from its public key.
The agent's libp2p node will then join the global UserAgent.ID P2P network. It will use the Kademlia DHT to announce its presence, associating its PeerID with its current network address (a multiaddr). This makes the agent discoverable by any other peer in the network that knows its PeerID.
This capability unlocks several powerful use cases:
 * Real-Time Control: A user's front-end application (e.g., the Persistence.Digital dashboard) can discover their active agent on the network and establish a direct, end-to-end encrypted channel to send commands or receive streaming updates and logs.
 * Inter-Agent Collaboration: Two different UserAgent.ID agents can be instructed to collaborate on a task. They can discover each other on the DHT and establish a direct communication channel to exchange data or coordinate actions, enabling complex, multi-agent workflows without a central intermediary.
 * Resilient Messaging: The decentralized nature of the network provides resilience. If centralized servers were to experience an outage, direct P2P communication channels could remain operational.
2.6 Layer 6: The Verifiable Persistence Ledger
This layer provides the immutable, tamper-evident audit log for all high-stakes actions approved by the user. The original concept of a "managed private blockchain"  is sound in its goal—to create a permanent, verifiable record—but the term "blockchain" carries unnecessary technical overhead and market connotations associated with cryptocurrencies.
2.6.1 Architecture: An Immutable, Merkle-ized Audit Log
A more precise and efficient implementation of this concept is an Immutable, Merkle-ized Audit Log. This architecture provides the same core security guarantees of immutability and non-repudiation as a private blockchain but without the complexity of a distributed consensus mechanism.
The process for creating this verifiable log is as follows:
 * Transaction Creation: Each time a user provides a cryptographic signature to approve an action (as described in Layer 2.2.2), this signed consent object becomes a transaction.
 * Block Formation: These transactions are collected and grouped into time-ordered blocks by the Persistence.Digital service.
 * Merkle Tree Construction: For each block, a Merkle tree is constructed from the hashes of all transactions within that block. This process recursively hashes pairs of nodes until a single root hash is produced. The Merkle root is a compact, cryptographic summary of the entire block; any change to any transaction in the block would result in a different Merkle root.
 * Chain Linking: This Merkle root is then cryptographically chained to the Merkle root of the previous block. This is achieved by creating a block header that contains the current block's Merkle root, a timestamp, and the hash of the previous block's header. This header is then signed by a service key controlled by Persistence.Digital.
This structure creates a tamper-evident chain. To alter a past transaction, an attacker would need to recompute the Merkle root for that block and all subsequent blocks, and also break the service's signature scheme. This provides a powerful and efficient mechanism for ensuring the integrity of the audit log.
For ultimate public verifiability, the Merkle root of each block can be periodically "anchored" to a public blockchain like Bitcoin or Ethereum. This can be done by embedding the hash in a transaction's OP_RETURN field or as data in a smart contract call. This process creates an immutable, publicly verifiable timestamp for the state of the audit log, proving that the records existed in a certain state at a certain time and have not been altered since.
3.0 Security, Scalability, and Compliance
A protocol designed to handle sensitive user data and execute autonomous actions requires a holistic and uncompromising approach to security, a clear strategy for achieving massive scale, and a design that embeds data privacy principles from its inception.
3.1 The End-to-End Security Model: A Zero-Trust Approach
The UserAgent.ID architecture is founded on a zero-trust security model, which assumes no implicit trust and continually verifies at every layer of the stack. This comprehensive strategy synthesizes the security measures from each architectural layer to provide defense-in-depth.
 * Identity and Authentication: The foundation of the security model is the user's non-custodial, sharded key managed via Web3Auth's MPC architecture. The service operator (Persistence.Digital) never has access to the user's complete private key and therefore cannot impersonate the user or decrypt their data without their active participation. Every session is authenticated through cryptographic challenges that require the user to prove control over their key shares.
 * Authorization and Consent: All high-stakes actions are explicitly authorized via the Cryptographic Consent Flow. The system operates on a principle of "deny by default," executing actions only upon receipt of a valid, verifiable digital signature from the user for a specific, proposed operation. This prevents unauthorized agent activity and creates a non-repudiable audit trail.
 * Execution Isolation: All agent workloads are executed within hardware-isolated Firecracker microVMs. This is the strongest pillar of the runtime security model. Even if an attacker were to find a remote code execution vulnerability in the browser or one of the automation tools, the exploit would be contained within the ephemeral microVM. The blast radius is limited to a single user's single session, and the attacker cannot break out to the host kernel or affect any other tenant's workload.
 * Data Persistence and Encryption: All user data stored in the Persistence Vault—including context history, automation recipes, and credentials—is encrypted at rest. The encryption keys are derived from the user's sovereign identity key. This ensures that the data is unintelligible to the service operator, maintaining data confidentiality even in the event of a physical data center breach.
 * Secure Communication: All real-time communication over the P2P fabric is end-to-end encrypted using libp2p's built-in secure channel protocols (such as Noise). This protects data in transit from eavesdropping or man-in-the-middle attacks.
 * Supply Chain Security: The root file systems used to boot the microVMs are built using container security best practices. This includes using minimal, hardened base images to reduce the attack surface, performing regular vulnerability scanning on all software packages, and removing unnecessary binaries or capabilities. The principle of least privilege is strictly enforced within the microVM's guest environment.
3.2 Architecture for Scale: Managing a Multi-Tenant MicroVM Fleet
The primary scalability challenge for Persistence.Digital is the efficient management of a large, highly dynamic fleet of Firecracker microVMs. The architecture must be able to instantiate and terminate thousands of these environments per second while ensuring fair resource allocation and preventing the "noisy neighbor" problem, where one tenant's high resource consumption degrades performance for others.
The proposed strategy leverages the mature and powerful ecosystem of Kubernetes as the macro-orchestration layer, combined with specific configurations to manage the unique demands of a microVM-based workload.
 * Orchestration with Kubernetes: Kubernetes will serve as the control plane for managing the underlying host machines. To manage Firecracker microVMs within this framework, a runtime that conforms to the Container Runtime Interface (CRI) will be used, such as Kata Containers or firecracker-containerd. This allows the Kubernetes kubelet to schedule a "pod" that is, in reality, a hardware-isolated microVM, abstracting the virtualization layer from the higher-level orchestration logic.
 * Tenant Isolation with Namespaces: Each tenant (or a group of tenants) will be assigned to a dedicated Kubernetes Namespace. Namespaces provide a scope for names and a mechanism for attaching authorization and policy to a subsection of the cluster. This provides logical isolation for tenant resources like pods, services, and secrets.
 * Resource Management with Quotas and Limits: To ensure fairness and prevent resource exhaustion, Kubernetes ResourceQuotas will be applied to each tenant's namespace. A ResourceQuota provides constraints that limit the aggregate resource consumption per namespace, setting hard limits on the total amount of CPU, memory, and storage that a tenant can use. Additionally, LimitRanges will be used to enforce minimum and maximum resource constraints on individual pods (microVMs), preventing tenants from requesting excessively large or small environments. If a container attempts to exceed its memory limit, it will be terminated; if it attempts to exceed its CPU limit, it will be throttled. This is the primary mechanism for mitigating the noisy neighbor problem.
 * Network Isolation with Network Policies: Security and isolation are further enhanced at the network level using Kubernetes NetworkPolicies. A default-deny policy will be established for all tenant namespaces, blocking all ingress and egress traffic by default. Then, specific policies will be added to explicitly allow necessary communication, such as allowing pods to communicate with internal platform services (e.g., the Persistence Vault) or to make egress connections to the public internet for browser automation tasks. This prevents any cross-tenant network traffic and severely restricts the potential for a compromised microVM to be used for lateral movement or network attacks.
3.3 Data Privacy by Design: A Strategy for GDPR and CCPA Compliance
The UserAgent.ID protocol is designed with data privacy as a core architectural principle, not an afterthought. Given that the system handles personal data and requires KYC for its advanced features, adherence to stringent data protection regulations like the EU's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) is mandatory.
The protocol's design inherently supports compliance by embedding key privacy principles :
 * Data Minimization and Purpose Limitation: The system is designed to collect and process only the data necessary for a specific, user-approved purpose. The structured format of automation recipes and the explicit nature of the consent flow ensure that both the user and the system have a clear understanding of what data is being accessed and why.
 * Transparency: The system's operations are designed to be transparent. The privacy policy will clearly articulate that AI systems are used to process user data. More importantly, the Verifiable Persistence Ledger provides a transparent and immutable audit trail of all high-stakes actions that the user has consented to, giving them a clear view into their agent's activities.
 * Privacy by Design and by Default: The zero-trust security model, particularly the use of non-custodial keys and end-to-end encryption, ensures that user data is protected by default. The service is architected in such a way that the operator cannot access the content of user data, a core tenet of privacy-centric design.
The architecture is specifically engineered to facilitate the fulfillment of key data subject rights under GDPR and CCPA :
 * Right to Access and Data Portability: All of a user's data—context, memories, and automations—is stored in a structured format within their personal Persistence Vault. The system will provide a straightforward function for users to export their entire vault in a machine-readable format (e.g., a collection of JSON and YAML files), fulfilling their right to access and portability.
 * Right to Erasure (Right to be Forgotten): This is a critical and often technically challenging right to implement. The UserAgent.ID architecture provides an elegant cryptographic solution. When a user invokes their right to be forgotten, the master key share associated with their identity can be securely destroyed. Since all their data at rest within the Persistence Vault is encrypted using keys derived from this master key, the destruction of the key renders all associated data permanently and irrecoverably unreadable. This is equivalent to cryptographic erasure. Following this, the encrypted data blobs and vector embeddings can be purged from the physical storage systems.
 * Legal Basis for Processing: The primary legal basis for processing personal data within the UserAgent.ID ecosystem will be explicit, informed consent. This consent is not obtained through a pre-ticked box but through the robust Cryptographic Consent Flow, which captures a specific, unambiguous, and auditable indication of the user's wishes for each significant processing activity.
4.0 Ecosystem and Go-to-Market Strategy
The success of UserAgent.ID hinges on a dual strategy: fostering a broad, open ecosystem around the core protocol while simultaneously building a compelling and sustainable commercial service on top of it. This section outlines the relationship between the open protocol and the commercial service, the go-to-market strategy for driving adoption, and the business model for Persistence.Digital.
4.1 The Open Protocol (UserAgent.ID) vs. The Commercial Service (Persistence.Digital)
The ecosystem is intentionally designed with a clear separation between the foundational, open protocol and the value-added commercial services built upon it.
 * UserAgent.ID (The Protocol): This is the open standard that will be governed by a foundation or a consortium of partners. It comprises the technical specifications for the .uaid handle and its resolution, the decentralized identity and cryptographic consent mechanisms (Layer 2), and the P2P communication fabric (Layer 5). The goal is to make UserAgent.ID a public good—a universal standard that any developer, LLM provider, or company can implement to make their services compatible with persistent, verifiable agents. Creating a UserAgent.ID handle and managing one's own identity will be free, encouraging mass adoption and network effects.
 * Persistence.Digital (The Service): This is the first and premier commercial implementation of the protocol, focusing on providing the more resource-intensive layers as a managed service. It offers the Secure Execution Environment (Layer 3) as a "Browser-as-a-Service" and the Persistence Vault (Layer 4) as a secure "digital brain." Persistence.Digital is the primary revenue-generating entity, providing the user-friendly interface, the reliable infrastructure, and the advanced features that make the protocol easily accessible and powerful for end-users and businesses.
4.2 A Developer-First GTM: Fostering an Ecosystem of Trusted Agents
The go-to-market (GTM) strategy for the entire ecosystem is fundamentally developer-first. The protocol's value increases exponentially with the number of applications, tools, and agents that support it. Therefore, the initial focus must be on winning the hearts and minds of the developers who will build the first generation of agentic applications. The strategy will be executed in three phases.
 * Phase 1: Foundation & Community Building:
   * Launch with Substance: The initial launch will be centered around the publication of this comprehensive technical whitepaper, accompanied by the open-sourcing of core client libraries (e.g., a reference implementation of the cryptographic consent flow, SDKs for interacting with the P2P network).
   * Target Early Adopters: Initial outreach will focus on communities that are philosophically and technically aligned with the project's vision. This includes AI/ML developers working with frameworks like LangChain and LlamaIndex, who are actively grappling with the challenges of agent state and tool use, and the Web3 developer community, which deeply understands the principles of decentralized identity and user sovereignty.
   * Content and Education: A steady stream of high-quality technical content, including blog posts, tutorials, and conference presentations, will be produced to explain the protocol's architecture and its benefits.
 * Phase 2: Platform Evangelism & Tooling:
   * Lower the Barrier to Entry: The focus will shift to creating an exceptional developer experience. This includes comprehensive documentation, interactive tutorials, and robust SDKs for popular programming languages and frameworks.
   * Launch a Developer Preview: Persistence.Digital will launch an invite-only developer preview, offering generous free credits, direct access to the engineering team for support, and a platform for feedback. This will seed the ecosystem with initial projects and gather valuable user insights.
   * Cultivate a Marketplace: A key initiative will be the creation of a public registry or marketplace for trusted, user-created automation "recipes." This will encourage community contribution, allow users to benefit from each other's work, and create a powerful network effect around the platform's capabilities.
 * Phase 3: Enterprise & Mainstream Adoption:
   * Develop Enterprise-Grade Features: As the platform matures, Persistence.Digital will build features specifically for business use cases. This includes team accounts for collaborative management of vaults and recipes, centralized billing and administration, advanced security auditing features, and guaranteed Service Level Agreements (SLAs).
   * Target Business Automation: The GTM focus will expand to target enterprises and SMBs that require trusted AI agents for automating internal business processes (e.g., data entry, report generation, customer support workflows) or for integration into their own customer-facing products.
4.3 The Persistence.Digital Business Model: Freemium and Subscription Tiers
Persistence.Digital will employ a classic Freemium SaaS business model. This model is highly effective for developer-focused and product-led growth strategies, as it allows users to experience the core value of the product for free, lowering the barrier to adoption and creating a natural funnel for conversion to paid plans.
The subscription tiers are carefully designed to align with the evolving needs of different user segments, from individual hobbyists to large enterprises. The primary value metrics that will scale across the tiers are the volume of persistent storage (context), the number of active automations, and the amount of compute time consumed in the secure execution environment.
<br>
Table 3: Persistence.Digital Subscription Tiers
| Tier | Price | Target Audience | Key Features |
|---|---|---|---|
| Free | $0 / month | Individuals, Developers, Hobbyists | • UserAgent.ID handle creation & management<br>• Limited context storage (e.g., last 1,000 interactions)<br>• Limited active automations (e.g., 5 recipes)<br>• Limited execution time (e.g., 100 minutes/month)<br>• Community support |
| Pro | $20 / month (Illustrative) | Power Users, Freelancers, Professionals | • All Free features, plus:<br>• Unlimited context storage<br>• Unlimited active automations<br>• Increased execution time (e.g., 2,000 minutes/month)<br>• Access to premium automation actions & integrations<br>• Priority email support |
| Team / Enterprise | Custom / Per-seat | Businesses, Development Teams, Enterprises | • All Pro features, plus:<br>• Team collaboration on shared vaults and recipes<br>• Centralized billing and user administration<br>• Advanced security controls and audit logs<br>• Role-Based Access Control (RBAC) for automations<br>• Dedicated support and Service Level Agreements (SLAs) |
<br>
5.0 Conclusion: The Path to a Universal Agentic Web
The UserAgent.ID protocol and the Persistence.Digital service represent more than just an incremental improvement in AI assistants; they are a fundamental reimagining of the architecture for human-AI collaboration. By systematically addressing the State-Action Gap—the critical deficiency in persistence, interoperability, and trust that holds back current AI systems—this blueprint lays the foundation for a new paradigm: the Agentic Web.
In this future, AI moves beyond its role as a passive, stateless information oracle and becomes an active, persistent, and verifiable partner in our digital lives. The protocol's core philosophy—grounded in utility, verified identity, and architecturally enforced user sovereignty—provides the necessary trust for users to delegate meaningful, high-stakes tasks to autonomous agents. The multi-layered architecture, from the non-custodial identity fabric powered by MPC to the hardware-isolated execution environments of Firecracker microVMs, is a testament to a security-first, zero-trust design philosophy.
UserAgent.ID is not envisioned as a single product but as a foundational, open layer upon which a new ecosystem of innovation can be built. It provides the universal addressing, identity, and communication standards necessary for a world of specialized, interoperable AI agents to emerge. By creating a clear separation between the open protocol and commercial service implementations like Persistence.Digital, the ecosystem can benefit from both the network effects of an open standard and the sustainable innovation driven by a viable business model.
The path forward requires collaboration, rigor, and a commitment to the core principles outlined in this document. The vision of a universal agentic web—where trusted, autonomous agents can securely and efficiently interact and transact on behalf of their users—is within reach. This whitepaper serves as both a detailed blueprint for its construction and a call to action for the developers, researchers, entrepreneurs, and visionaries who will build it. The next era of the internet will be defined by agency, and UserAgent.ID provides the key to unlocking it.
